{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714ee47a-4fa9-4699-8039-a8c12c25b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc5d58e-fc13-42ef-899b-c264c6db3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "| id|Airline|Flight|AirportFrom|AirportTo|DayOfWeek|Time|Length|Delay|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "|  1|     CO|   269|        SFO|      IAH|        3|  15|   205|    1|\n",
      "|  2|     US|  1558|        PHX|      CLT|        3|  15|   222|    1|\n",
      "|  3|     AA|  2400|        LAX|      DFW|        3|  20|   165|    1|\n",
      "|  4|     AA|  2466|        SFO|      DFW|        3|  20|   195|    1|\n",
      "|  5|     AS|   108|        ANC|      SEA|        3|  30|   202|    0|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# starting spark session\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"Flight Delay Prediction\").getOrCreate()\n",
    "spark = (SparkSession.builder.appName(\"Flight Delay Prediction\") \\\n",
    "         .config(\"spark.executor.memory\", \"4g\") \\\n",
    "         .config(\"spark.driver.memory\", \"4g\") \\\n",
    "         .getOrCreate())\n",
    "\n",
    "# Loading the airline dataset\n",
    "df = spark.read.csv(\"Airlines.csv\",header=True,inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9519b9c8-3a64-448d-9fdc-58af8b004056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "| id|Airline|Flight|AirportFrom|AirportTo|DayOfWeek|Time|Length|Delay|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "|  1|     CO|   269|        SFO|      IAH|        3|  15|   205|    1|\n",
      "|  2|     US|  1558|        PHX|      CLT|        3|  15|   222|    1|\n",
      "|  3|     AA|  2400|        LAX|      DFW|        3|  20|   165|    1|\n",
      "|  4|     AA|  2466|        SFO|      DFW|        3|  20|   195|    1|\n",
      "|  5|     AS|   108|        ANC|      SEA|        3|  30|   202|    0|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to standardize airline codes\n",
    "def standardize_airline_codes(df):\n",
    "    return df.withColumn(\"Airline\", \n",
    "                         when(col(\"Airline\").isin([\"AS\", \"ASA\"]), \"AS\")\n",
    "                         .when(col(\"Airline\").isin([\"AA\", \"AAL\"]), \"AA\")\n",
    "                         .when(col(\"Airline\").isin([\"AC\", \"ACA\"]), \"AC\")\n",
    "                         .when(col(\"Airline\").isin([\"AM\", \"AMX\"]), \"AM\")\n",
    "                         .when(col(\"Airline\").isin([\"CO\", \"COA\"]), \"CO\")\n",
    "                         .when(col(\"Airline\").isin([\"DL\", \"DAL\"]), \"DL\")\n",
    "                         .when(col(\"Airline\").isin([\"FX\", \"FDX\"]), \"FX\")\n",
    "                         .when(col(\"Airline\").isin([\"HA\", \"HAL\"]), \"HA\")\n",
    "                         .when(col(\"Airline\").isin([\"NW\", \"NWA\"]), \"NW\")\n",
    "                         .when(col(\"Airline\").isin([\"PO\", \"PAC\"]), \"PO\")\n",
    "                         .when(col(\"Airline\").isin([\"SW\", \"SWA\"]), \"SW\")\n",
    "                         .when(col(\"Airline\").isin([\"UA\", \"UAL\"]), \"UA\")\n",
    "                         .when(col(\"Airline\") == \"5X\", \"5X\")\n",
    "                         .when(col(\"Airline\").isin([\"VS\", \"VIR\"]), \"VS\")\n",
    "                         .when(col(\"Airline\").isin([\"VB\", \"VIV\"]), \"VB\")\n",
    "                         .when(col(\"Airline\").isin([\"WS\", \"WJ\"]), \"WS\")\n",
    "                         .otherwise(col(\"Airline\")))\n",
    "\n",
    "# Apply the function to standardize airline codes\n",
    "df_standardized = standardize_airline_codes(df)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_standardized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "705ca877-700f-49dd-a42d-a23fbf4fea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------+---------+---------+----+------+-----+-----+\n",
      "| id|Airline|Flight|AirportFrom|AirportTo|DayOfWeek|Time|Length|Delay|count|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+-----+\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Any duplicate values? replace\n",
    "duplicate_rows = df.groupBy(df.columns)\\\n",
    "    .count()\\\n",
    "    .where(col('count') > 1)\n",
    "\n",
    "# Show duplicate rows\n",
    "duplicate_rows.show()\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5553834e-8057-4361-be59-59894e7d331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "| id|Airline|Flight|AirportFrom|AirportTo|DayOfWeek|Time|Length|Delay|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "|  0|      0|     0|          0|        0|        0|   0|     0|    0|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the number of missing values for each column\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Show the count of missing values per column\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7f2be1-f40d-4104-8cf2-328e5048cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------+---------+---------+----+------+-----+----+------+-------------+-----------------+---------------+---------------+-------------------+-----------------+--------------------+\n",
      "| id|Airline|Flight|AirportFrom|AirportTo|DayOfWeek|Time|Length|Delay|Hour|Minute|Airline_Index|AirportFrom_Index|AirportTo_Index|Airline_Encoded|AirportFrom_Encoded|AirportTo_Encoded|            features|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+----+------+-------------+-----------------+---------------+---------------+-------------------+-----------------+--------------------+\n",
      "|176|     9E|  3955|        LBB|      MEM|        3| 355|   119|    0|   3|    55|         11.0|            108.0|           25.0|(17,[11],[1.0])|  (292,[108],[1.0])| (292,[25],[1.0])|(603,[0,1,13,127,...|\n",
      "|526|     B6|   742|        SJU|      MCO|        3| 361|   184|    0|   3|    61|         12.0|             60.0|           11.0|(17,[12],[1.0])|   (292,[60],[1.0])| (292,[11],[1.0])|(603,[0,1,14,79,3...|\n",
      "|566|     CO|  1831|        PBI|      IAH|        3| 365|   163|    0|   3|    65|          9.0|             53.0|            5.0| (17,[9],[1.0])|   (292,[53],[1.0])|  (292,[5],[1.0])|(603,[0,1,11,72,3...|\n",
      "|718|     9E|  3997|        MLI|      MEM|        3| 375|    96|    0|   3|    75|         11.0|            129.0|           25.0|(17,[11],[1.0])|  (292,[129],[1.0])| (292,[25],[1.0])|(603,[0,1,13,148,...|\n",
      "|994|     9E|  4251|        RDU|      CVG|        3| 390|   100|    0|   3|    90|         11.0|             37.0|           31.0|(17,[11],[1.0])|   (292,[37],[1.0])| (292,[31],[1.0])|(603,[0,1,13,56,3...|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+----+------+-------------+-----------------+---------------+---------------+-------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|Delay|\n",
      "+--------------------+-----+\n",
      "|(603,[0,1,13,127,...|    0|\n",
      "|(603,[0,1,14,79,3...|    0|\n",
      "|(603,[0,1,11,72,3...|    0|\n",
      "|(603,[0,1,13,148,...|    0|\n",
      "|(603,[0,1,13,56,3...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, minute, dayofweek\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Assuming 'Time' is in HHMM integer format\n",
    "df = df.withColumn(\"Hour\", (col(\"Time\") / 100).cast(\"integer\"))\n",
    "df = df.withColumn(\"Minute\", (col(\"Time\") % 100).cast(\"integer\"))\n",
    "\n",
    "# Encode categorical variables\n",
    "indexer = StringIndexer(inputCols=[\"Airline\", \"AirportFrom\", \"AirportTo\"], outputCols=[\"Airline_Index\", \"AirportFrom_Index\", \"AirportTo_Index\"])\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"Airline_Index\", \"AirportFrom_Index\", \"AirportTo_Index\"], outputCols=[\"Airline_Encoded\", \"AirportFrom_Encoded\", \"AirportTo_Encoded\"])\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "# Assemble all features into one vector column for modeling\n",
    "assembler = VectorAssembler(inputCols=[\"Hour\", \"Minute\", \"Airline_Encoded\", \"AirportFrom_Encoded\", \"AirportTo_Encoded\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "# Show the processed DataFrame\n",
    "df.select(\"features\", \"Delay\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84813607-a8f4-443f-ab6b-020c96b273a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 22:42:15 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------+---------+---------+----+------+-----+----+------+-------------+-----------------+---------------+---------------+-------------------+-----------------+--------------------+\n",
      "| id|Airline|Flight|AirportFrom|AirportTo|DayOfWeek|Time|Length|Delay|Hour|Minute|Airline_Index|AirportFrom_Index|AirportTo_Index|Airline_Encoded|AirportFrom_Encoded|AirportTo_Encoded|            features|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+----+------+-------------+-----------------+---------------+---------------+-------------------+-----------------+--------------------+\n",
      "|176|     9E|  3955|        LBB|      MEM|        3| 355|   119|    0|   3|    55|         11.0|            108.0|           25.0|(17,[11],[1.0])|  (292,[108],[1.0])| (292,[25],[1.0])|(603,[0,1,13,127,...|\n",
      "|526|     B6|   742|        SJU|      MCO|        3| 361|   184|    0|   3|    61|         12.0|             60.0|           11.0|(17,[12],[1.0])|   (292,[60],[1.0])| (292,[11],[1.0])|(603,[0,1,14,79,3...|\n",
      "|566|     CO|  1831|        PBI|      IAH|        3| 365|   163|    0|   3|    65|          9.0|             53.0|            5.0| (17,[9],[1.0])|   (292,[53],[1.0])|  (292,[5],[1.0])|(603,[0,1,11,72,3...|\n",
      "|718|     9E|  3997|        MLI|      MEM|        3| 375|    96|    0|   3|    75|         11.0|            129.0|           25.0|(17,[11],[1.0])|  (292,[129],[1.0])| (292,[25],[1.0])|(603,[0,1,13,148,...|\n",
      "|994|     9E|  4251|        RDU|      CVG|        3| 390|   100|    0|   3|    90|         11.0|             37.0|           31.0|(17,[11],[1.0])|   (292,[37],[1.0])| (292,[31],[1.0])|(603,[0,1,13,56,3...|\n",
      "+---+-------+------+-----------+---------+---------+----+------+-----+----+------+-------------+-----------------+---------------+---------------+-------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|Delay|\n",
      "+--------------------+-----+\n",
      "|(603,[0,1,13,127,...|    0|\n",
      "|(603,[0,1,14,79,3...|    0|\n",
      "|(603,[0,1,11,72,3...|    0|\n",
      "|(603,[0,1,13,148,...|    0|\n",
      "|(603,[0,1,13,56,3...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf8b4b4-2f1e-4182-8443-75569aeacb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 377864\n",
      "Test Dataset Count: 161519\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting\n",
    "\n",
    "(trainingData, testData) = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b1f91e-7c93-498a-b59c-5326eb501c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 22:42:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/15 22:42:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ffrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Initialize the classifiers\n",
    "classifiers = [\n",
    "    LogisticRegression(featuresCol='features', labelCol='Delay'),\n",
    "    DecisionTreeClassifier(featuresCol='features', labelCol='Delay'),\n",
    "    RandomForestClassifier(featuresCol='features', labelCol='Delay'),\n",
    "    GBTClassifier(featuresCol='features', labelCol='Delay')\n",
    "]\n",
    "\n",
    "# Prepare an evaluator for different metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Delay', predictionCol=\"prediction\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    # Set up the parameter grid\n",
    "    paramGrid = ParamGridBuilder().build()  # Customize grids for each model if needed\n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = CrossValidator(estimator=classifier,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "\n",
    "    # Fit model\n",
    "    cvModel = cv.fit(trainingData)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = cvModel.transform(testData)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Store results\n",
    "    results.append((classifier.__class__.__name__, accuracy, f1, precision, recall))\n",
    "\n",
    "# Output results\n",
    "for result in results:\n",
    "    print(f\"Model: {result[0]}, Accuracy: {result[1]:.2f}, F1: {result[2]:.2f}, Precision: {result[3]:.2f}, Recall: {result[4]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4f107d-fc29-4555-8957-cbb25fd0bd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.6868242171859389\n",
      "F1 Score:  0.633762688272908\n",
      "Precision:  0.6422725744686333\n",
      "Recall:  0.6433670342188844\n"
     ]
    }
   ],
   "source": [
    "#LLOGISTIC REGRESSION\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Binary Classification Evaluator for ROC-AUC or area under PR\n",
    "binaryEvaluator = BinaryClassificationEvaluator(labelCol='Delay')\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy: \", accuracy)\n",
    "\n",
    "# Multiclass Classification Evaluator for F1, Precision, Recall\n",
    "multiEvaluator = MulticlassClassificationEvaluator(labelCol='Delay', metricName='f1')\n",
    "f1_score = multiEvaluator.evaluate(predictions)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Changing the metric name for precision and recall\n",
    "precision = multiEvaluator.setMetricName('weightedPrecision').evaluate(predictions)\n",
    "recall = multiEvaluator.setMetricName('weightedRecall').evaluate(predictions)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c28d1ab6-23a0-4490-a567-c51102431180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solving the same thing using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d388f1-9674-49f4-8e78-05218d199917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/67/9jzhnw_d2j114qjt6hzwr8s80000gn/T/ipykernel_2091/2727287456.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/var/folders/67/9jzhnw_d2j114qjt6hzwr8s80000gn/T/ipykernel_2091/2727287456.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')  # Forward fill for simplicity\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('Airlines.csv')\n",
    "\n",
    "# Handling missing values by imputation or removal\n",
    "df = df.fillna(method='ffill')  # Forward fill for simplicity\n",
    "\n",
    "# Removing duplicates if any\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b8b3dd1-3f9c-4ae7-b76f-91a5e2d330b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Selecting features and target\n",
    "features = ['Airline', 'Flight', 'AirportFrom', 'AirportTo', 'DayOfWeek', 'Time']  # example features\n",
    "target = 'Delay'  # Target variable\n",
    "\n",
    "# Creating a pipeline for transforming data\n",
    "categorical_features = ['Airline', 'AirportFrom', 'AirportTo']\n",
    "numeric_features = ['DayOfWeek', 'Time']  # Assuming these are the only numeric features\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combining preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Applying preprocessing to training data\n",
    "X_train = preprocessor.fit_transform(train_df[features])\n",
    "y_train = train_df[target].values\n",
    "\n",
    "# Applying preprocessing to testing data\n",
    "X_test = preprocessor.transform(test_df[features])\n",
    "y_test = test_df[target].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e70a54-b5b6-452f-baa8-0800781afae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance: {'Accuracy': 0.6458559285111748, 'F1 Score': 0.637416749335298, 'ROC AUC': 0.6909410382872345, 'Precision': 0.6439162365744134, 'Recall': 0.6458559285111748}\n",
      "Random Forest Performance: {'Accuracy': 0.6177683843636734, 'F1 Score': 0.6169814437827008, 'ROC AUC': 0.6545554382762797, 'Precision': 0.6165126457561193, 'Recall': 0.6177683843636734}\n",
      "Gradient Boosting Performance: {'Accuracy': 0.6491559832030924, 'F1 Score': 0.626767416598852, 'ROC AUC': 0.6947004780612609, 'Precision': 0.658614737293002, 'Recall': 0.6491559832030924}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Training and evaluating the classifiers\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    # Create a pipeline with the preprocessor and classifier\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', clf)])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(train_df[features], train_df[target])\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(test_df[features])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(test_df[target], y_pred),\n",
    "        'F1 Score': f1_score(test_df[target], y_pred, average='weighted'),\n",
    "        'ROC AUC': roc_auc_score(test_df[target], pipeline.predict_proba(test_df[features])[:, 1]),\n",
    "        'Precision': precision_score(test_df[target], y_pred, average='weighted'),\n",
    "        'Recall': recall_score(test_df[target], y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"{result} Performance: {results[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceff71f-09a0-4dc8-a48f-01c4b134c5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
